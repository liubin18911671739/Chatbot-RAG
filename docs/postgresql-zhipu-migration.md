# PostgreSQLæ•°æ®åº“è¿ç§»ä¸æ™ºæ™®æ¸…è¨€APIé›†æˆæ–¹æ¡ˆ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›äº†ä»å½“å‰Flask+FAISS+Google Gemini/DeepSeekæŠ€æœ¯æ ˆè¿ç§»åˆ°FastAPI+PostgreSQL+pgvector+æ™ºæ™®æ¸…è¨€çš„å®Œæ•´å®æ–½æ–¹æ¡ˆã€‚

### ğŸ¯ è¿ç§»ç›®æ ‡
- **æ•°æ®åº“**: MySQL â†’ PostgreSQL 16+ with pgvector
- **å‘é‡å­˜å‚¨**: FAISS â†’ pgvector
- **LLMæœåŠ¡**: Google Gemini/DeepSeek â†’ æ™ºæ™®æ¸…è¨€ (Zhipu AI)
- **è®¤è¯æ–¹å¼**: RADIUS â†’ é‚®ç®±è®¤è¯ + JWT
- **ç½‘ç»œé™åˆ¶**: æ ¡å›­ç½‘é™åˆ¶ â†’ å¼€æ”¾è®¿é—®

---

## 1. PostgreSQLç¯å¢ƒæ­å»º

### 1.1 Docker Composeé…ç½® (`docker-compose.postgres.yml`)

```yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: rag-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: rag_bot
      POSTGRES_USER: rag_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-rag_password_2024}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
      - ./backups:/backups
    ports:
      - "5432:5432"
    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag_user -d rag_bot"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: rag-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - rag-network
    command: redis-server --appendonly yes

  pgadmin:
    image: dpage/pgadmin4
    container_name: rag-pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@ragbot.com
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD:-admin123}
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      - postgres
    networks:
      - rag-network

volumes:
  postgres_data:
  redis_data:
  pgadmin_data:

networks:
  rag-network:
    driver: bridge
```

### 1.2 ç¯å¢ƒå˜é‡é…ç½® (`.env`)

```bash
# PostgreSQLé…ç½®
POSTGRES_PASSWORD=rag_password_2024
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=rag_bot
POSTGRES_USER=rag_user

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0

# æ™ºæ™®æ¸…è¨€APIé…ç½®
ZHIPU_API_KEY=your_zhipu_api_key_here
ZHIPU_EMBEDDING_MODEL=embedding-3
ZHIPU_CHAT_MODEL=glm-4

# åº”ç”¨é…ç½®
DATABASE_URL=postgresql://rag_user:rag_password_2024@localhost:5432/rag_bot
JWT_SECRET_KEY=your_jwt_secret_key_here
JWT_ALGORITHM=HS256
JWT_EXPIRE_HOURS=24

# å¼€å‘æ¨¡å¼
DEBUG=true
ENVIRONMENT=development
```

### 1.3 å¯åŠ¨è„šæœ¬ (`scripts/start-postgres.sh`)

```bash
#!/bin/bash

echo "ğŸš€ å¯åŠ¨PostgreSQLæ•°æ®åº“..."

# æ£€æŸ¥Dockeræ˜¯å¦è¿è¡Œ
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Dockeræœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨Docker"
    exit 1
fi

# åˆ›å»ºç½‘ç»œï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
docker network create rag-network 2>/dev/null || true

# å¯åŠ¨æœåŠ¡
docker-compose -f docker-compose.postgres.yml up -d

echo "â³ ç­‰å¾…PostgreSQLå¯åŠ¨..."
sleep 10

# æ£€æŸ¥è¿æ¥çŠ¶æ€
if docker exec rag-postgres pg_isready -U rag_user -d rag_bot; then
    echo "âœ… PostgreSQLå¯åŠ¨æˆåŠŸï¼"
    echo "ğŸ“Š PgAdminè®¿é—®: http://localhost:5050"
    echo "ğŸ—„ï¸ æ•°æ®åº“è¿æ¥: postgresql://rag_user:rag_password_2024@localhost:5432/rag_bot"
else
    echo "âŒ PostgreSQLå¯åŠ¨å¤±è´¥"
    exit 1
fi

# è¿è¡Œåˆå§‹åŒ–è„šæœ¬
echo "ğŸ”§ è¿è¡Œæ•°æ®åº“åˆå§‹åŒ–..."
docker exec rag-postgres psql -U rag_user -d rag_bot -c "\i /docker-entrypoint-initdb.d/01-init-extensions.sql"
docker exec rag-postgres psql -U rag_user -d rag_bot -c "\i /docker-entrypoint-initdb.d/02-create-tables.sql"

echo "âœ… æ•°æ®åº“ç¯å¢ƒæ­å»ºå®Œæˆï¼"
```

---

## 2. pgvectoræ‰©å±•å®‰è£…é…ç½®

### 2.1 æ‰©å±•åˆå§‹åŒ–è„šæœ¬ (`migrations/01-init-extensions.sql`)

```sql
-- PostgreSQL + pgvector åˆå§‹åŒ–è„šæœ¬
-- åˆ›å»ºå¿…è¦çš„æ‰©å±•

-- å¯ç”¨pgvectoræ‰©å±•
CREATE EXTENSION IF NOT EXISTS vector;
COMMENT ON EXTENSION vector IS 'å‘é‡ç›¸ä¼¼åº¦æœç´¢æ‰©å±•';

-- å¯ç”¨uuid-osspæ‰©å±•ï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€IDï¼‰
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
COMMENT ON EXTENSION "uuid-ossp" IS 'UUIDç”Ÿæˆæ‰©å±•';

-- å¯ç”¨pg_trgmæ‰©å±•ï¼ˆç”¨äºæ–‡æœ¬æœç´¢ï¼‰
CREATE EXTENSION IF NOT EXISTS pg_trgm;
COMMENT ON EXTENSION pg_trgm IS 'å…¨æ–‡æœç´¢æ‰©å±•';

-- å¯ç”¨btree_ginæ‰©å±•ï¼ˆç”¨äºç´¢å¼•ä¼˜åŒ–ï¼‰
CREATE EXTENSION IF NOT EXISTS btree_gin;
COMMENT ON EXTENSION btree_gin IS 'B-tree GINç´¢å¼•æ‰©å±•';

-- åˆ›å»ºå‘é‡ç›¸ä¼¼åº¦å‡½æ•°
CREATE OR REPLACE FUNCTION cosine_similarity(vec1 vector, vec2 vector)
RETURNS REAL AS $$
BEGIN
    RETURN 1 - (vec1 <=> vec2);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- åˆ›å»ºæ–‡æ¡£ç›¸ä¼¼åº¦ç´¢å¼•ç±»å‹
CREATE OR REPLACE FUNCTION document_similarity(vec1 vector, vec2 vector)
RETURNS REAL AS $$
BEGIN
    RETURN 1 - (vec1 <=> vec2);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- åˆ›å»ºå‘é‡æ“ä½œç¬¦
CREATE OPERATOR <=> (vector, vector) RETURNS float4 AS
'SELECT pgvector_native_distance($1, $2);';

-- åˆ›å»ºå‘é‡ç´¢å¼•æ”¯æŒ
CREATE OPERATOR CLASS vector_l2_ops FOR TYPE vector USING ivfflat
AS
    OPERATOR 1 <=> (vector, vector),
    OPERATOR 2 <=> (vector, vector),
    OPERATOR 3 <=> (vector, vector);

-- åˆ›å»ºå‘é‡æ“ä½œç¬¦ç±»æ³¨é‡Š
COMMENT ON OPERATOR CLASS vector_l2_ops IS 'å‘é‡L2è·ç¦»æ“ä½œç¬¦ç±»';

-- è®¾ç½®é»˜è®¤æœç´¢ç­–ç•¥
ALTER DATABASE rag_bot SET default_text_search_config = 'simple';

-- åˆ›å»ºä¸­æ–‡æœç´¢é…ç½®
CREATE TEXT SEARCH CONFIGURATION chinese (COPY = simple);
ALTER TEXT SEARCH CONFIGURATION chinese
    ALTER MAPPING FOR asciiword, asciihword, hword_asciiprefix, hword, hword_part
    WITH simple;

-- åˆ›å»ºå‘é‡ç´¢å¼•æ€§èƒ½ç›‘æ§è§†å›¾
CREATE OR REPLACE VIEW vector_index_stats AS
SELECT
    schemaname,
    tablename,
    indexname,
    indexdef,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size,
    pg_relation_size(indexrelid) as index_size_bytes
FROM pg_indexes
WHERE indexdef LIKE '%vector%';

-- æ€§èƒ½ä¼˜åŒ–è®¾ç½®
ALTER SYSTEM SET max_parallel_workers_per_gather = 4;
ALTER SYSTEM SET max_parallel_workers = 8;
ALTER SYSTEM SET shared_buffers = '256MB';
ALTER SYSTEM SET effective_cache_size = '1GB';

-- æäº¤äº‹åŠ¡
COMMIT;
```

### 2.2 æ€§èƒ½ä¼˜åŒ–é…ç½®

```sql
-- æ€§èƒ½ä¼˜åŒ–è®¾ç½®
-- å»ºè®®çš„ç”Ÿäº§ç¯å¢ƒé…ç½®

-- 1. å†…å­˜ç›¸å…³é…ç½®
SET shared_preload_libraries = 'pg_stat_statements, vector';
SET track_activity_query_size = 2048;

-- 2. è¿æ¥é…ç½®
SET max_connections = 200;
SET superuser_reserved_connections = 10;

-- 3. æŸ¥è¯¢ä¼˜åŒ–
SET work_mem = '64MB';
SET maintenance_work_mem = '256MB';

-- 4. å‘é‡ç´¢å¼•ä¼˜åŒ–
SET ivfflat.probe_limit = 2000;
SET hnsw.ef_construction = 200;
SET hnsw.ef_search = 100;

-- 5. è‡ªåŠ¨åˆ†æè®¾ç½®
SET autovacuum = on;
SET autovacuum_max_workers = 3;
SET autovacuum_naptime = '1min';

-- 6. ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
SET track_counts = on;
SET track_io_timing = on;
```

---

## 3. æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬

### 3.1 è¡¨ç»“æ„åˆ›å»º (`migrations/02-create-tables.sql`)

```sql
-- RAGé—®ç­”æœºå™¨äººæ•°æ®åº“è¡¨ç»“æ„
-- æ”¯æŒé‚®ç®±è®¤è¯ã€å‘é‡å­˜å‚¨ã€RAGæµæ°´çº¿

-- ç”¨æˆ·è®¤è¯ç›¸å…³è¡¨
CREATE SCHEMA IF NOT EXISTS auth;

-- ç”¨æˆ·è¡¨ï¼ˆæ›¿ä»£åŸæœ‰çš„RADIUSè®¤è¯ï¼‰
CREATE TABLE auth.users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    name VARCHAR(100) NOT NULL,
    avatar_url TEXT,
    phone VARCHAR(20),
    department VARCHAR(100),
    student_id VARCHAR(50),
    role VARCHAR(20) DEFAULT 'student' CHECK (role IN ('student', 'teacher', 'admin', 'super_admin')),
    is_active BOOLEAN DEFAULT true,
    is_verified BOOLEAN DEFAULT false,
    is_premium BOOLEAN DEFAULT false,
    email_verified_at TIMESTAMP,
    email_verification_token VARCHAR(255),
    password_reset_token VARCHAR(255),
    password_reset_expires_at TIMESTAMP,
    two_factor_enabled BOOLEAN DEFAULT false,
    two_factor_secret VARCHAR(32),
    language VARCHAR(10) DEFAULT 'zh-CN',
    timezone VARCHAR(50) DEFAULT 'Asia/Shanghai',
    preferences JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_login_at TIMESTAMP,
    login_count INTEGER DEFAULT 0
);

-- ç”¨æˆ·ä¼šè¯è¡¨
CREATE TABLE auth.sessions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    token_hash VARCHAR(255) NOT NULL,
    device_info JSONB,
    ip_address INET,
    user_agent TEXT,
    is_active BOOLEAN DEFAULT true,
    expires_at TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_accessed_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- RAGç›¸å…³è¡¨
CREATE SCHEMA IF NOT EXISTS rag;

-- çŸ¥è¯†åº“è¡¨
CREATE TABLE rag.knowledge_bases (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(50) DEFAULT 'general',
    scene_id VARCHAR(50),
    is_active BOOLEAN DEFAULT true,
    is_public BOOLEAN DEFAULT false,
    owner_id UUID REFERENCES auth.users(id) ON DELETE CASCADE,
    embedding_model VARCHAR(50) DEFAULT 'text-embedding-ada-002',
    chunk_size INTEGER DEFAULT 1000,
    chunk_overlap INTEGER DEFAULT 200,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    created_by UUID REFERENCES auth.users(id),
    updated_by UUID REFERENCES auth.users(id)
);

-- æ–‡æ¡£è¡¨
CREATE TABLE rag.documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    knowledge_base_id UUID NOT NULL REFERENCES rag.knowledge_bases(id) ON DELETE CASCADE,
    title VARCHAR(500) NOT NULL,
    content TEXT NOT NULL,
    file_path TEXT,
    file_type VARCHAR(50),
    file_size BIGINT,
    original_filename VARCHAR(255),
    mime_type VARCHAR(100),
    status VARCHAR(20) DEFAULT 'processing' CHECK (status IN ('processing', 'completed', 'failed')),
    processing_progress INTEGER DEFAULT 0,
    processing_error TEXT,
    metadata JSONB DEFAULT '{}',
    word_count INTEGER DEFAULT 0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    processed_at TIMESTAMP WITH TIME ZONE,
    created_by UUID REFERENCES auth.users(id),
    updated_by UUID REFERENCES auth.users(id)
);

-- æ–‡æ¡£å‘é‡ç‰‡æ®µè¡¨
CREATE TABLE rag.document_chunks (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES rag.documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    vector vector(1536),
    embedding_model VARCHAR(50) DEFAULT 'text-embedding-ada-002',
    word_count INTEGER DEFAULT 0,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- å¯¹è¯è¡¨
CREATE SCHEMA IF NOT EXISTS chat;

-- å¯¹è¯ä¼šè¯è¡¨
CREATE TABLE chat.conversations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,
    title VARCHAR(500),
    scene_id VARCHAR(50),
    is_active BOOLEAN DEFAULT true,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æ¶ˆæ¯è¡¨
CREATE TABLE chat.messages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    conversation_id UUID NOT NULL REFERENCES chat.conversations(id) ON DELETE CASCADE,
    role VARCHAR(20) NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    sources JSONB DEFAULT '[]',
    token_count INTEGER DEFAULT 0,
    model_name VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- æ¶ˆæ¯å‘é‡è¡¨ï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
CREATE TABLE chat.message_embeddings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    message_id UUID NOT NULL REFERENCES chat.messages(id) ON DELETE CASCADE,
    vector vector(1536),
    embedding_model VARCHAR(50) DEFAULT 'text-embedding-3-small',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- åˆ›å»ºå‘é‡ç´¢å¼•
CREATE INDEX idx_document_chunks_vector ON rag.document_chunks USING ivfflat (vector vector_l2_ops);

-- åˆ›å»ºæ—¶é—´ç´¢å¼•
CREATE INDEX idx_documents_created_at ON rag.documents (created_at);
CREATE INDEX idx_conversations_created_at ON chat.conversations (created_at);
CREATE INDEX idx_messages_created_at ON chat.messages (created_at);
CREATE INDEX idx_sessions_expires_at ON auth.sessions (expires_at);

-- åˆ›å»ºç”¨æˆ·ç›¸å…³ç´¢å¼•
CREATE INDEX idx_users_email ON auth.users (email);
CREATE INDEX idx_users_role ON auth.users (role);
CREATE INDEX idx_sessions_user_id ON auth.sessions (user_id);
CREATE INDEX idx_sessions_active ON auth.sessions (is_active);

-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX idx_kb_owner_active ON rag.knowledge_bases (owner_id, is_active);
CREATE INDEX idx_docs_kb_status ON rag.documents (knowledge_base_id, status);
CREATE INDEX idx_chunks_doc_index ON rag.document_chunks (document_id, chunk_index);
CREATE INDEX idx_conv_user_active ON chat.conversations (user_id, is_active);
CREATE INDEX idx_msgs_conv_created ON chat.messages (conversation_id, created_at);

-- åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•
CREATE INDEX idx_docs_title_gin ON rag.documents USING gin (to_tsvector('simple', title));
CREATE INDEX idx_docs_content_gin ON rag.documents USING gin (to_tsvector('simple', content));
CREATE INDEX idx_msgs_content_gin ON chat.messages USING gin (to_tsvector('simple', content));

-- æ’å…¥åˆå§‹æ•°æ®
INSERT INTO rag.knowledge_bases (id, name, description, category, scene_id, is_public, created_by) VALUES
    ('550e8400-e29b-41d4-a716-446655440000', 'é€šç”¨åŠ©æ‰‹', 'é€šç”¨æ™ºèƒ½é—®ç­”åŠ©æ‰‹', 'general', 'general', true, '00000000-0000-0000-0000-000000000000'),
    ('550e8400-e29b-41d4-a716-446655440001', 'æ€æ”¿å­¦ä¹ ç©ºé—´', 'æ€æƒ³æ”¿æ²»æ•™è‚²èµ„æºåº“', 'education', 'db_sizheng', true, '00000000-0000-0000-0000-000000000000'),
    ('550e8400-e29b-41d4-a716-446655440002', 'å­¦ä¹ æŒ‡å¯¼', 'å­¦ä¹ æ–¹æ³•å’ŒæŒ‡å¯¼èµ„æº', 'education', 'db_xuexizhidao', true, '00000000-0000-0000-0000-000000000000'),
    ('550e8400-e29b-41d4-a716-446655440003', 'æ™ºæ…§æ€æ”¿', 'æ™ºèƒ½åŒ–æ€æƒ³æ”¿æ²»æ•™è‚²', 'education', 'db_zhihuisizheng', true, '00000000-0000-0000-0000-000000000000'),
    ('550e8400-e29b-41d4-716-446655440004', 'ç§‘ç ”è¾…åŠ©', 'ç§‘ç ”æ–¹æ³•å’Œå·¥å…·', 'research', 'db_keyanfuzhu', true, '00000000-0000-0000-0000-000000000000'),
    ('550e8400-e29b-41d4-716-446655440005', 'ç½‘ä¸ŠåŠäº‹å…', 'æ ¡å›­è¡Œæ”¿æœåŠ¡', 'service', 'db_wangshangbanshiting', true, '00000000-0000-0000-0000-000000000000');

-- åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·ï¼ˆå¯†ç ï¼šadmin123ï¼‰
INSERT INTO auth.users (id, email, password_hash, name, role, is_active, is_verified, created_by) VALUES
    ('00000000-0000-0000-0000-000000000001', 'admin@ragbot.com', '$2b$12$QqY8J/7t6.7kOI/Y8qLOHwJxgMu5.jUGYL.n8/Hb6ZHT7e9XIXuy3ebQxF', 'ç³»ç»Ÿç®¡ç†å‘˜', 'super_admin', true, true, '00000000-0000-0000-0000-000000000001');

COMMIT;
```

### 3.3 æ•°æ®è¿ç§»å·¥å…·ç±» (`migrations/migration_tools.py`)

```python
import asyncio
import asyncpg
import os
from typing import List, Dict, Any
from datetime import datetime
import json
import hashlib

class DatabaseMigration:
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.pool = None

    async def __aenter__(self):
        self.pool = await asyncpg.create_pool(
            self.db_url,
            min_size=2,
            max_size=10,
            command_timeout=60
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.pool:
            await self.pool.close()

    async def migrate_from_mysql(self, mysql_config: Dict[str, Any]):
        """ä»MySQLè¿ç§»æ•°æ®åˆ°PostgreSQL"""
        print("ğŸ”„ å¼€å§‹æ•°æ®è¿ç§»...")

        # è¿æ¥MySQLï¼ˆéœ€è¦å®‰è£…pymysqlï¼‰
        import pymysql
        mysql_conn = pymysql.connect(**mysql_config)

        try:
            # è¿ç§»ç”¨æˆ·æ•°æ®
            await self._migrate_users(mysql_conn)

            # è¿ç§»å¯¹è¯æ•°æ®
            await self._migrate_conversations(mysql_conn)

            # è¿ç§»æ¶ˆæ¯æ•°æ®
            await self._migrate_messages(mysql_conn)

            # è¿ç§»æ–‡æ¡£æ•°æ®
            await self._migrate_documents(mysql_conn)

            print("âœ… æ•°æ®è¿ç§»å®Œæˆï¼")

        finally:
            mysql_conn.close()

    async def _migrate_users(self, mysql_conn):
        """è¿ç§»ç”¨æˆ·æ•°æ®"""
        print("ğŸ‘¥ è¿ç§»ç”¨æˆ·æ•°æ®...")

        cursor = mysql_conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM users")
        users = cursor.fetchall()

        for user in users:
            # è½¬æ¢ç”¨æˆ·è§’è‰²
            role_map = {
                'student': 'student',
                'teacher': 'teacher',
                'admin': 'admin'
            }
            role = role_map.get(user.get('role', 'student'), 'student')

            # æ£€æŸ¥é‚®ç®±æ˜¯å¦å·²å­˜åœ¨
            existing = await self.pool.fetchrow(
                "SELECT id FROM auth.users WHERE email = $1",
                (user['email'],)
            )

            if not existing:
                await self.pool.execute(
                    """
                    INSERT INTO auth.users (
                        id, email, password_hash, name, phone, department,
                        student_id, role, is_active, is_verified, created_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
                    """,
                    (
                        user.get('id', self._generate_uuid()),
                        user['email'],
                        user.get('password_hash', ''),
                        user.get('name', ''),
                        user.get('phone'),
                        user.get('department'),
                        user.get('student_id'),
                        role,
                        True,
                        True,
                        user.get('created_at', datetime.now())
                    )
                )

    async def _migrate_documents(self, mysql_conn):
        """è¿ç§»æ–‡æ¡£å’Œå‘é‡æ•°æ®"""
        print("ğŸ“„ è¿ç§»æ–‡æ¡£æ•°æ®...")

        # è·å–FAISSå‘é‡æ•°æ®
        import faiss
        import numpy as np

        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„FAISSç´¢å¼•è·¯å¾„è¿›è¡Œè°ƒæ•´
        faiss_index_path = "/path/to/faiss/index.faiss"

        cursor = mysql_conn.cursor(dictionary=True)
        cursor.execute("SELECT * FROM documents")
        documents = cursor.fetchall()

        if os.path.exists(faiss_index_path):
            index = faiss.read_index(faiss_index_path)

            for i, doc in enumerate(documents):
                # æ’å…¥æ–‡æ¡£
                doc_id = self._generate_uuid()

                await self.pool.execute(
                    """
                    INSERT INTO rag.documents (
                        id, knowledge_base_id, title, content, file_path, file_type,
                        status, word_count, created_at, created_by
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    """,
                    (
                        doc_id,
                        self._get_kb_id_for_document(doc),
                        doc.get('title', ''),
                        doc.get('content', ''),
                        doc.get('file_path', ''),
                        doc.get('file_type', ''),
                        'completed',
                        doc.get('word_count', 0),
                        doc.get('created_at', datetime.now()),
                        doc.get('created_by', self._get_user_id_for_doc(doc))
                    )
                )

                # åˆ†å—å¤„ç†æ–‡æ¡£å†…å®¹
                content = doc.get('content', '')
                chunk_size = 1000
                chunk_overlap = 200

                for j in range(0, len(content), chunk_size - chunk_overlap):
                    chunk_start = j
                    chunk_end = min(j + chunk_size, len(content))
                    chunk_content = content[chunk_start:chunk_end]

                    # è·å–å¯¹åº”çš„å‘é‡
                    vector = index.reconstruct([i])[0]
                    if len(vector) > 1536:
                        vector = vector[:1536]
                    elif len(vector) < 1536:
                        vector = np.pad(vector, (0, 1536 - len(vector)))

                    # æ’å…¥å‘é‡ç‰‡æ®µ
                    await self.pool.execute(
                        """
                        INSERT INTO rag.document_chunks (
                            id, document_id, chunk_index, content, vector
                        ) VALUES ($1, $2, $3, $4, $5)
                        """,
                        (
                            self._generate_uuid(),
                            doc_id,
                            j // (chunk_size - chunk_overlap),
                            chunk_content,
                            vector.tolist()
                        )
                    )

    def _generate_uuid(self) -> str:
        """ç”ŸæˆUUID"""
        import uuid
        return str(uuid.uuid4())

    def _get_kb_id_for_document(self, document: Dict) -> str:
        """è·å–æ–‡æ¡£å¯¹åº”çš„çŸ¥è¯†åº“ID"""
        # æ ¹æ®æ–‡æ¡£çš„ç±»åˆ«æˆ–åœºæ™¯è¿”å›å¯¹åº”çš„çŸ¥è¯†åº“ID
        category = document.get('category', 'general')
        scene_id = document.get('scene_id', 'general')

        kb_mapping = {
            'sizheng': '550e8400-e29b-41d4-a716-446655440001',
            'xuexizhidao': '550e8400-e29b-41d4-a716-446655440002',
            'zhihuisizheng': '550e400-e29b-41d4-716-446655440003',
            'keyanfuzhu': '550e400-e29b-41d4-716-446655440004',
            'wangshangbanshiting': '550e400-e29b-41d4-716-446655440005',
        }

        return kb_mapping.get(scene_id, '550e400-e29b-41d4-a716-446655440000')

    def _get_user_id_for_doc(self, document: Dict) -> str:
        """è·å–æ–‡æ¡£åˆ›å»ºè€…ID"""
        # å¦‚æœæœ‰ç”¨æˆ·IDæ˜ å°„è¡¨ï¼Œå¯ä»¥ä½¿ç”¨
        return document.get('user_id', '00000000-0000-0000-0000-000000000001')
```

### 3.4 æ•°æ®è¿ç§»è„šæœ¬ (`scripts/migrate-data.py`)

```python
#!/usr/bin/env python3
import asyncio
import os
from dotenv import load_dotenv
from migration_tools import DatabaseMigration

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

async def main():
    """ä¸»è¿ç§»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿæ•°æ®è¿ç§»...")

    # è·å–ç¯å¢ƒé…ç½®
    db_url = os.getenv('DATABASE_URL')
    if not db_url:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°DATABASE_URLç¯å¢ƒå˜é‡")
        return

    mysql_config = {
        'host': os.getenv('MYSQL_HOST', 'localhost'),
        'port': int(os.getenv('MYSQL_PORT', 3306)),
        'user': os.getenv('MYSQL_USER', 'root'),
        'password': os.getenv('MYSQL_PASSWORD', ''),
        'database': os.getenv('MYSQL_DATABASE', 'chatbot_rag'),
    }

    faiss_path = os.getenv('FAISS_INDEX_PATH', './faiss_index')

    try:
        # åˆ›å»ºè¿ç§»å·¥å…·
        migration = DatabaseMigration(db_url, mysql_config, faiss_path)

        # æ‰§è¡Œè¿ç§»æ­¥éª¤
        await migration.migrate_users()
        print("âœ… ç”¨æˆ·æ•°æ®è¿ç§»å®Œæˆ")

        await migration.migrate_conversations()
        print("âœ… å¯¹è¯å†å²è¿ç§»å®Œæˆ")

        await migration.migrate_faiss_index()
        print("âœ… FAISSå‘é‡æ•°æ®è¿ç§»å®Œæˆ")

        print("ğŸ‰ æ‰€æœ‰æ•°æ®è¿ç§»å®Œæˆï¼")

        # éªŒè¯è¿ç§»ç»“æœ
        await migration.validate_migration()
        print("âœ… è¿ç§»éªŒè¯é€šè¿‡")

    except Exception as e:
        print(f"âŒ è¿ç§»å¤±è´¥: {e}")
        return 1

    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
```

## 4. æ™ºæ™®æ¸…è¨€APIé›†æˆ

### 4.1 ç¯å¢ƒé…ç½®

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env
# æ™ºæ™®æ¸…è¨€APIé…ç½®
ZHIPU_API_KEY=your_zhipu_api_key_here
ZHIPU_BASE_URL=https://open.bigmodel.cn/api/paas/v4
ZHIPU_EMBEDDING_MODEL=embedding-2
ZHIPU_CHAT_MODEL=glm-4-plus
ZHIPU_CHAT_MODEL_FAST=glm-4-flash

# å‘é‡é…ç½®
EMBEDDING_DIMENSION=1024
VECTOR_INDEX_TYPE=ivfflat
VECTOR_LISTS=100
VECTOR_PROBES=10

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0
REDIS_TTL=3600

# åº”ç”¨é…ç½®
APP_ENV=development
DEBUG=true
LOG_LEVEL=INFO
```

### 4.2 æ™ºæ™®æ¸…è¨€APIæœåŠ¡å°è£…

#### ZhipuAIServiceç±»
```python
# services/zhipu_service.py
import asyncio
import json
import time
from typing import Dict, List, Optional, Any, AsyncGenerator
import aiohttp
import numpy as np
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class ZhipuAIService:
    """æ™ºæ™®æ¸…è¨€APIæœåŠ¡å°è£…"""

    def __init__(self, api_key: str, base_url: str = "https://open.bigmodel.cn/api/paas/v4"):
        self.api_key = api_key
        self.base_url = base_url
        self.session = None
        self._last_request_time = 0
        self._rate_limit_delay = 0.1  # 100ms between requests

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    def _rate_limit(func):
        """è¯·æ±‚é¢‘ç‡é™åˆ¶è£…é¥°å™¨"""
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            current_time = time.time()
            time_since_last = current_time - self._last_request_time
            if time_since_last < self._rate_limit_delay:
                await asyncio.sleep(self._rate_limit_delay - time_since_last)

            self._last_request_time = time.time()
            return await func(self, *args, **kwargs)
        return wrapper

    def _get_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´"""
        return {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    @_rate_limit
    async def get_embedding(self, text: str, model: str = "embedding-2") -> List[float]:
        """
        è·å–æ–‡æœ¬åµŒå…¥å‘é‡

        Args:
            text: è¾“å…¥æ–‡æœ¬
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            å‘é‡åˆ—è¡¨
        """
        if not self.session:
            raise RuntimeError("Session not initialized. Use async with statement.")

        url = f"{self.base_url}/embeddings"
        payload = {
            "model": model,
            "input": text
        }

        try:
            async with self.session.post(url, headers=self._get_headers(), json=payload) as response:
                response.raise_for_status()
                data = await response.json()

                if "data" not in data or not data["data"]:
                    raise ValueError("Invalid embedding response")

                return data["data"][0]["embedding"]

        except aiohttp.ClientError as e:
            logger.error(f"Embedding API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Embedding processing failed: {e}")
            raise

    @_rate_limit
    async def get_embeddings_batch(self, texts: List[str], model: str = "embedding-2") -> List[List[float]]:
        """
        æ‰¹é‡è·å–åµŒå…¥å‘é‡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        # æ™ºæ™®æ¸…è¨€æ”¯æŒæ‰¹é‡è¯·æ±‚ï¼Œé™åˆ¶æ¯æ¬¡æœ€å¤š100ä¸ªæ–‡æœ¬
        batch_size = 100
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            url = f"{self.base_url}/embeddings"
            payload = {
                "model": model,
                "input": batch
            }

            try:
                async with self.session.post(url, headers=self._get_headers(), json=payload) as response:
                    response.raise_for_status()
                    data = await response.json()

                    if "data" not in data:
                        raise ValueError("Invalid batch embedding response")

                    batch_embeddings = [item["embedding"] for item in data["data"]]
                    all_embeddings.extend(batch_embeddings)

            except aiohttp.ClientError as e:
                logger.error(f"Batch embedding API request failed: {e}")
                raise
            except Exception as e:
                logger.error(f"Batch embedding processing failed: {e}")
                raise

        return all_embeddings

    @_rate_limit
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = "glm-4-plus",
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        èŠå¤©å¯¹è¯è¡¥å…¨

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¿”å›

        Returns:
            APIå“åº”
        """
        if not self.session:
            raise RuntimeError("Session not initialized. Use async with statement.")

        url = f"{self.base_url}/chat/completions"
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream
        }

        if max_tokens:
            payload["max_tokens"] = max_tokens

        try:
            async with self.session.post(url, headers=self._get_headers(), json=payload) as response:
                response.raise_for_status()

                if stream:
                    return self._handle_stream_response(response)
                else:
                    data = await response.json()
                    return data

        except aiohttp.ClientError as e:
            logger.error(f"Chat completion API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Chat completion processing failed: {e}")
            raise

    async def _handle_stream_response(self, response) -> AsyncGenerator[Dict[str, Any], None]:
        """å¤„ç†æµå¼å“åº”"""
        async for line in response.content:
            line = line.decode('utf-8').strip()
            if line.startswith('data: '):
                data_str = line[6:]
                if data_str == '[DONE]':
                    break
                try:
                    data = json.loads(data_str)
                    yield data
                except json.JSONDecodeError:
                    continue

    @_rate_limit
    async def semantic_search(
        self,
        query: str,
        documents: List[str],
        model: str = "embedding-2"
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            ç›¸ä¼¼åº¦æ’åºçš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = await self.get_embedding(query, model)

            # è·å–æ–‡æ¡£å‘é‡
            doc_embeddings = await self.get_embeddings_batch(documents, model)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = []
            query_np = np.array(query_embedding)

            for i, doc_embedding in enumerate(doc_embeddings):
                doc_np = np.array(doc_embedding)
                similarity = np.dot(query_np, doc_np) / (
                    np.linalg.norm(query_np) * np.linalg.norm(doc_np)
                )
                similarities.append({
                    "index": i,
                    "document": documents[i],
                    "similarity": float(similarity)
                })

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x["similarity"], reverse=True)
            return similarities

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            raise
```

#### Redisç¼“å­˜æœåŠ¡
```python
# services/redis_cache.py
import json
import asyncio
from typing import Any, Optional, List, Dict
import aioredis
import pickle
import hashlib
from datetime import timedelta

class RedisCache:
    """Redisç¼“å­˜æœåŠ¡"""

    def __init__(self, redis_url: str, default_ttl: int = 3600):
        self.redis_url = redis_url
        self.default_ttl = default_ttl
        self.redis = None

    async def connect(self):
        """è¿æ¥Redis"""
        self.redis = await aioredis.from_url(self.redis_url, decode_responses=False)

    async def disconnect(self):
        """æ–­å¼€Redisè¿æ¥"""
        if self.redis:
            await self.redis.close()

    def _get_key(self, prefix: str, identifier: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"rag:{prefix}:{identifier}"

    def _hash_data(self, data: Any) -> str:
        """ç”Ÿæˆæ•°æ®å“ˆå¸Œ"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()

    async def get(self, prefix: str, identifier: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if not self.redis:
            await self.connect()

        key = self._get_key(prefix, identifier)
        try:
            data = await self.redis.get(key)
            if data:
                return pickle.loads(data)
        except Exception as e:
            logger.error(f"Cache get failed: {e}")
        return None

    async def set(self, prefix: str, identifier: str, data: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        if not self.redis:
            await self.connect()

        key = self._get_key(prefix, identifier)
        ttl = ttl or self.default_ttl

        try:
            serialized_data = pickle.dumps(data)
            await self.redis.setex(key, ttl, serialized_data)
            return True
        except Exception as e:
            logger.error(f"Cache set failed: {e}")
            return False

    async def delete(self, prefix: str, identifier: str) -> bool:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        if not self.redis:
            await self.connect()

        key = self._get_key(prefix, identifier)
        try:
            await self.redis.delete(key)
            return True
        except Exception as e:
            logger.error(f"Cache delete failed: {e}")
            return False

    async def get_embedding(self, text: str, model: str = "embedding-2") -> Optional[List[float]]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡ç¼“å­˜"""
        text_hash = self._hash_data({"text": text, "model": model})
        return await self.get("embedding", text_hash)

    async def set_embedding(self, text: str, embedding: List[float], model: str = "embedding-2") -> bool:
        """ç¼“å­˜æ–‡æœ¬åµŒå…¥å‘é‡"""
        text_hash = self._hash_data({"text": text, "model": model})
        # å‘é‡ç¼“å­˜7å¤©
        return await self.set("embedding", text_hash, embedding, ttl=604800)

    async def get_search_results(self, query_hash: str) -> Optional[List[Dict[str, Any]]]:
        """è·å–æœç´¢ç»“æœç¼“å­˜"""
        return await self.get("search", query_hash)

    async def set_search_results(self, query_hash: str, results: List[Dict[str, Any]], ttl: int = 1800) -> bool:
        """ç¼“å­˜æœç´¢ç»“æœ"""
        return await self.set("search", query_hash, results, ttl)

    async def invalidate_pattern(self, pattern: str) -> int:
        """æ ¹æ®æ¨¡å¼åˆ é™¤ç¼“å­˜"""
        if not self.redis:
            await self.connect()

        try:
            keys = await self.redis.keys(f"rag:{pattern}:*")
            if keys:
                return await self.redis.delete(*keys)
        except Exception as e:
            logger.error(f"Cache invalidate pattern failed: {e}")
        return 0
```

### 4.3 RAGæœåŠ¡é‡æ„ï¼ˆPostgreSQL + pgvectorç‰ˆæœ¬ï¼‰

#### VectorServiceç±»
```python
# services/vector_service.py
import asyncio
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
import asyncpg
from contextlib import asynccontextmanager
import logging

logger = logging.getLogger(__name__)

class VectorService:
    """å‘é‡æœç´¢æœåŠ¡ - åŸºäºPostgreSQL pgvector"""

    def __init__(self, pool: asyncpg.Pool):
        self.pool = pool

    @asynccontextmanager
    async def get_connection(self):
        async with self.pool.acquire() as conn:
            yield conn

    async def create_vector_index(
        self,
        table_name: str,
        vector_column: str,
        index_type: str = "ivfflat",
        lists: int = 100
    ) -> bool:
        """
        åˆ›å»ºå‘é‡ç´¢å¼•

        Args:
            table_name: è¡¨å
            vector_column: å‘é‡åˆ—å
            index_type: ç´¢å¼•ç±»å‹ (ivfflat/hnsw)
            lists: IVFFlatç´¢å¼•çš„èšç±»æ•°é‡

        Returns:
            åˆ›å»ºæ˜¯å¦æˆåŠŸ
        """
        try:
            async with self.get_connection() as conn:
                if index_type.lower() == "ivfflat":
                    await conn.execute(
                        f"""
                        CREATE INDEX IF NOT EXISTS {table_name}_{vector_column}_idx
                        ON {table_name} USING ivfflat ({vector_column} vector_cosine_ops)
                        WITH (lists = {lists})
                        """
                    )
                elif index_type.lower() == "hnsw":
                    await conn.execute(
                        f"""
                        CREATE INDEX IF NOT EXISTS {table_name}_{vector_column}_idx
                        ON {table_name} USING hnsw ({vector_column} vector_cosine_ops)
                        """
                    )
                else:
                    raise ValueError(f"Unsupported index type: {index_type}")

                logger.info(f"Created {index_type} index on {table_name}.{vector_column}")
                return True

        except Exception as e:
            logger.error(f"Failed to create vector index: {e}")
            return False

    async def vector_search(
        self,
        query_vector: List[float],
        knowledge_base_id: Optional[str] = None,
        limit: int = 10,
        similarity_threshold: float = 0.7,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡ç›¸ä¼¼åº¦æœç´¢

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            knowledge_base_id: çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            async with self.get_connection() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_conditions = ["1 = 1"]
                params = {"query_vector": query_vector, "limit": limit}

                if knowledge_base_id:
                    where_conditions.append("kb_id = $knowledge_base_id")
                    params["knowledge_base_id"] = knowledge_base_id

                if filters:
                    if filters.get("status"):
                        where_conditions.append("status = $status")
                        params["status"] = filters["status"]
                    if filters.get("created_by"):
                        where_conditions.append("created_by = $created_by")
                        params["created_by"] = filters["created_by"]

                where_clause = " AND ".join(where_conditions)

                query = f"""
                SELECT
                    id, document_id, content,
                    1 - (vector <=> $query_vector::vector) as similarity,
                    chunk_index, metadata
                FROM rag.document_chunks
                WHERE {where_clause}
                AND 1 - (vector <=> $query_vector::vector) >= $similarity
                ORDER BY similarity DESC
                LIMIT $limit
                """

                params["similarity"] = similarity_threshold

                rows = await conn.fetch(query, *params.values())

                results = []
                for row in rows:
                    result = {
                        "id": row["id"],
                        "document_id": row["document_id"],
                        "content": row["content"],
                        "similarity": float(row["similarity"]),
                        "chunk_index": row["chunk_index"],
                        "metadata": row["metadata"]
                    }
                    results.append(result)

                return results

        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            raise

    async def hybrid_search(
        self,
        query_text: str,
        query_vector: List[float],
        knowledge_base_id: Optional[str] = None,
        limit: int = 10,
        vector_weight: float = 0.7,
        keyword_weight: float = 0.3,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            query_vector: æŸ¥è¯¢å‘é‡
            knowledge_base_id: çŸ¥è¯†åº“ID
            limit: è¿”å›ç»“æœæ•°é‡
            vector_weight: å‘é‡æœç´¢æƒé‡
            keyword_weight: å…³é”®è¯æœç´¢æƒé‡
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æ··åˆæœç´¢ç»“æœ
        """
        try:
            async with self.get_connection() as conn:
                # å‘é‡æœç´¢
                vector_results = await self.vector_search(
                    query_vector, knowledge_base_id, limit * 2, 0.1, filters
                )

                # å…³é”®è¯æœç´¢
                keyword_results = await self.keyword_search(
                    query_text, knowledge_base_id, limit * 2, filters
                )

                # åˆå¹¶å’Œé‡æ’åºç»“æœ
                combined_scores = {}

                # å¤„ç†å‘é‡æœç´¢ç»“æœ
                for result in vector_results:
                    doc_id = result["document_id"]
                    vector_score = result["similarity"]
                    combined_scores[doc_id] = {
                        "vector_score": vector_score,
                        "keyword_score": 0.0,
                        "data": result
                    }

                # å¤„ç†å…³é”®è¯æœç´¢ç»“æœ
                for result in keyword_results:
                    doc_id = result["document_id"]
                    keyword_score = result["rank_score"]

                    if doc_id in combined_scores:
                        combined_scores[doc_id]["keyword_score"] = keyword_score
                    else:
                        combined_scores[doc_id] = {
                            "vector_score": 0.0,
                            "keyword_score": keyword_score,
                            "data": result
                        }

                # è®¡ç®—æœ€ç»ˆåˆ†æ•°
                final_results = []
                for doc_id, scores in combined_scores.items():
                    final_score = (
                        vector_weight * scores["vector_score"] +
                        keyword_weight * scores["keyword_score"]
                    )

                    result = scores["data"].copy()
                    result["final_score"] = final_score
                    final_results.append(result)

                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
                final_results.sort(key=lambda x: x["final_score"], reverse=True)
                return final_results[:limit]

        except Exception as e:
            logger.error(f"Hybrid search failed: {e}")
            raise

    async def keyword_search(
        self,
        query_text: str,
        knowledge_base_id: Optional[str] = None,
        limit: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å…³é”®è¯æœç´¢

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            knowledge_base_id: çŸ¥è¯†åº“ID
            limit: è¿”å›ç»“æœæ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            å…³é”®è¯æœç´¢ç»“æœ
        """
        try:
            async with self.get_connection() as conn:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                where_conditions = ["content ILIKE $query_text"]
                params = {
                    "query_text": f"%{query_text}%",
                    "limit": limit
                }

                if knowledge_base_id:
                    where_conditions.append("kb_id = $knowledge_base_id")
                    params["knowledge_base_id"] = knowledge_base_id

                if filters:
                    if filters.get("status"):
                        where_conditions.append("status = $status")
                        params["status"] = filters["status"]

                where_clause = " AND ".join(where_conditions)

                query = f"""
                SELECT
                    id, document_id, content,
                    ts_rank_cd(to_tsvector('chinese', content), plainto_tsquery('chinese', $query_text)) as rank_score,
                    chunk_index, metadata
                FROM rag.document_chunks
                WHERE {where_clause}
                ORDER BY rank_score DESC
                LIMIT $limit
                """

                rows = await conn.fetch(query, *params.values())

                results = []
                for row in rows:
                    result = {
                        "id": row["id"],
                        "document_id": row["document_id"],
                        "content": row["content"],
                        "rank_score": float(row["rank_score"]),
                        "chunk_index": row["chunk_index"],
                        "metadata": row["metadata"]
                    }
                    results.append(result)

                return results

        except Exception as e:
            logger.error(f"Keyword search failed: {e}")
            raise

    async def insert_chunks(
        self,
        chunks: List[Dict[str, Any]]
    ) -> List[str]:
        """
        æ‰¹é‡æ’å…¥æ–‡æ¡£ç‰‡æ®µ

        Args:
            chunks: ç‰‡æ®µåˆ—è¡¨

        Returns:
            æ’å…¥çš„ç‰‡æ®µIDåˆ—è¡¨
        """
        try:
            async with self.get_connection() as conn:
                chunk_ids = []

                for chunk in chunks:
                    # æ’å…¥æˆ–æ›´æ–°æ–‡æ¡£ä¿¡æ¯
                    await conn.execute(
                        """
                        INSERT INTO rag.documents (
                            id, kb_id, title, content, file_path,
                            file_type, status, word_count, created_at, created_by
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            content = EXCLUDED.content,
                            updated_at = CURRENT_TIMESTAMP
                        """,
                        chunk["document_id"],
                        chunk["kb_id"],
                        chunk["title"],
                        chunk["content"],
                        chunk.get("file_path", ""),
                        chunk.get("file_type", "text"),
                        "completed",
                        len(chunk["content"]),
                        chunk.get("created_at", "NOW()"),
                        chunk.get("created_by", "system")
                    )

                    # æ’å…¥å‘é‡ç‰‡æ®µ
                    chunk_id = await conn.fetchval(
                        """
                        INSERT INTO rag.document_chunks (
                            id, document_id, chunk_index, content, vector, metadata
                        ) VALUES ($1, $2, $3, $4, $5, $6)
                        RETURNING id
                        """,
                        chunk["id"],
                        chunk["document_id"],
                        chunk["chunk_index"],
                        chunk["content"],
                        chunk["vector"],
                        chunk.get("metadata", {})
                    )

                    chunk_ids.append(chunk_id)

                return chunk_ids

        except Exception as e:
            logger.error(f"Failed to insert chunks: {e}")
            raise

    async def delete_document(self, document_id: str) -> bool:
        """
        åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰ç‰‡æ®µ

        Args:
            document_id: æ–‡æ¡£ID

        Returns:
            åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            async with self.get_connection() as conn:
                # åˆ é™¤æ–‡æ¡£ç‰‡æ®µ
                await conn.execute(
                    "DELETE FROM rag.document_chunks WHERE document_id = $1",
                    document_id
                )

                # åˆ é™¤æ–‡æ¡£
                result = await conn.execute(
                    "DELETE FROM rag.documents WHERE id = $1",
                    document_id
                )

                return result == "DELETE 1"

        except Exception as e:
            logger.error(f"Failed to delete document: {e}")
            return False
```

#### RAGServiceç±»
```python
# services/rag_service.py
import asyncio
import hashlib
from typing import List, Dict, Any, Optional, AsyncGenerator
from services.zhipu_service import ZhipuAIService
from services.vector_service import VectorService
from services.redis_cache import RedisCache
from contextlib import asynccontextmanager
import logging

logger = logging.getLogger(__name__)

class RAGService:
    """RAGæœåŠ¡ - åŸºäºæ™ºæ™®æ¸…è¨€å’Œpgvector"""

    def __init__(
        self,
        zhipu_service: ZhipuAIService,
        vector_service: VectorService,
        cache_service: RedisCache
    ):
        self.zhipu_service = zhipu_service
        self.vector_service = vector_service
        self.cache = cache_service

    async def search_documents(
        self,
        query: str,
        knowledge_base_id: Optional[str] = None,
        limit: int = 10,
        search_type: str = "hybrid",
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            knowledge_base_id: çŸ¥è¯†åº“ID
            limit: è¿”å›ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ (vector/keyword/hybrid)
            filters: è¿‡æ»¤æ¡ä»¶

        Returns:
            æœç´¢ç»“æœ
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œç”¨äºç¼“å­˜
            query_hash = hashlib.md5(
                f"{query}_{knowledge_base_id}_{limit}_{search_type}_{filters}".encode()
            ).hexdigest()

            # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
            cached_results = await self.cache.get_search_results(query_hash)
            if cached_results:
                logger.info(f"Cache hit for query: {query[:50]}...")
                return cached_results

            # è·å–æŸ¥è¯¢å‘é‡
            query_vector = await self.zhipu_service.get_embedding(query)

            # æ ¹æ®æœç´¢ç±»å‹æ‰§è¡Œæœç´¢
            if search_type == "vector":
                results = await self.vector_service.vector_search(
                    query_vector, knowledge_base_id, limit, 0.7, filters
                )
            elif search_type == "keyword":
                results = await self.vector_service.keyword_search(
                    query, knowledge_base_id, limit, filters
                )
            else:  # hybrid
                results = await self.vector_service.hybrid_search(
                    query, query_vector, knowledge_base_id, limit,
                    vector_weight=0.7, keyword_weight=0.3, filters=filters
                )

            # ç¼“å­˜æœç´¢ç»“æœ
            await self.cache.set_search_results(query_hash, results, ttl=1800)

            return results

        except Exception as e:
            logger.error(f"Document search failed: {e}")
            raise

    async def generate_response(
        self,
        query: str,
        conversation_id: Optional[str] = None,
        knowledge_base_id: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        use_context: bool = True
    ) -> Dict[str, Any] | AsyncGenerator:
        """
        ç”ŸæˆRAGå“åº”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            conversation_id: å¯¹è¯ID
            knowledge_base_id: çŸ¥è¯†åº“ID
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            use_context: æ˜¯å¦ä½¿ç”¨ä¸Šä¸‹æ–‡æ£€ç´¢

        Returns:
            ç”Ÿæˆçš„å“åº”
        """
        try:
            # æœç´¢ç›¸å…³æ–‡æ¡£
            context_docs = []
            if use_context:
                context_docs = await self.search_documents(
                    query, knowledge_base_id, limit=5, search_type="hybrid"
                )

            # æ„å»ºä¸Šä¸‹æ–‡
            context_text = ""
            if context_docs:
                context_chunks = []
                for i, doc in enumerate(context_docs):
                    chunk = f"èµ„æ–™{i+1}: {doc['content']}\n"
                    context_chunks.append(chunk)
                    if len(''.join(context_chunks)) > 2000:  # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                        break

                context_text = '\n'.join(context_chunks)

            # æ„å»ºæ¶ˆæ¯
            messages = self._build_messages(query, context_text, conversation_id)

            if stream:
                return self._stream_response(
                    messages, temperature, max_tokens, context_docs
                )
            else:
                return await self._generate_response(
                    messages, temperature, max_tokens, context_docs
                )

        except Exception as e:
            logger.error(f"Response generation failed: {e}")
            raise

    def _build_messages(
        self,
        query: str,
        context: str,
        conversation_id: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€‚è¯·åŸºäºæä¾›çš„èµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„èµ„æ–™è¿›è¡Œå›ç­”ï¼Œå¦‚æœèµ„æ–™ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
2. å›ç­”è¦å‡†ç¡®ã€æ¸…æ™°ã€æœ‰æ¡ç†
3. å¦‚æœèµ„æ–™ä¸­æœ‰çŸ›ç›¾çš„ä¿¡æ¯ï¼Œè¯·æŒ‡å‡º
4. é€‚å½“ä½¿ç”¨é¡¹ç›®ç¬¦å·å’Œåˆ†æ®µæ¥ç»„ç»‡å›ç­”
5. è¯­è¨€è¦è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯"""

        user_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}"

        if context:
            user_prompt = f"å‚è€ƒèµ„æ–™ï¼š\n{context}\n\n{user_prompt}"

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        return messages

    async def _generate_response(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: Optional[int],
        context_docs: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´å“åº”"""
        response = await self.zhipu_service.chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )

        return {
            "response": response["choices"][0]["message"]["content"],
            "context_docs": context_docs,
            "model": response["model"],
            "usage": response.get("usage", {}),
            "sources": [doc["document_id"] for doc in context_docs]
        }

    async def _stream_response(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: Optional[int],
        context_docs: List[Dict[str, Any]]
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼ç”Ÿæˆå“åº”"""
        async for chunk in self.zhipu_service.chat_completion(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True
        ):
            yield {
                "chunk": chunk,
                "context_docs": context_docs if not chunk.get("choices") else None,
                "sources": [doc["document_id"] for doc in context_docs]
            }

    async def index_document(
        self,
        content: str,
        title: str,
        knowledge_base_id: str,
        file_path: Optional[str] = None,
        file_type: str = "text",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> str:
        """
        ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“

        Args:
            content: æ–‡æ¡£å†…å®¹
            title: æ–‡æ¡£æ ‡é¢˜
            knowledge_base_id: çŸ¥è¯†åº“ID
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹
            chunk_size: åˆ†å—å¤§å°
            chunk_overlap: åˆ†å—é‡å 

        Returns:
            æ–‡æ¡£ID
        """
        try:
            import uuid
            document_id = str(uuid.uuid4())

            # åˆ†å—å¤„ç†
            chunks = []
            for i in range(0, len(content), chunk_size - chunk_overlap):
                chunk_start = i
                chunk_end = min(i + chunk_size, len(content))
                chunk_content = content[chunk_start:chunk_end]

                chunks.append({
                    "content": chunk_content,
                    "chunk_index": i // (chunk_size - chunk_overlap)
                })

            # æ‰¹é‡è·å–åµŒå…¥å‘é‡
            chunk_texts = [chunk["content"] for chunk in chunks]
            embeddings = await self.zhipu_service.get_embeddings_batch(chunk_texts)

            # å‡†å¤‡æ’å…¥æ•°æ®
            insert_chunks = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                insert_chunks.append({
                    "id": str(uuid.uuid4()),
                    "document_id": document_id,
                    "kb_id": knowledge_base_id,
                    "title": title,
                    "content": chunk["content"],
                    "chunk_index": chunk["chunk_index"],
                    "vector": embedding,
                    "file_path": file_path,
                    "file_type": file_type,
                    "created_by": "system"
                })

            # æ’å…¥å‘é‡æ•°æ®åº“
            await self.vector_service.insert_chunks(insert_chunks)

            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            await self.cache.invalidate_pattern("search")

            logger.info(f"Successfully indexed document: {title}")
            return document_id

        except Exception as e:
            logger.error(f"Document indexing failed: {e}")
            raise

    async def delete_document(self, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            success = await self.vector_service.delete_document(document_id)

            if success:
                # æ¸…é™¤ç›¸å…³ç¼“å­˜
                await self.cache.invalidate_pattern("search")
                logger.info(f"Successfully deleted document: {document_id}")

            return success

        except Exception as e:
            logger.error(f"Document deletion failed: {e}")
            return False
```

### 4.4 æœåŠ¡é…ç½®å’Œåˆå§‹åŒ–

#### æœåŠ¡å·¥å‚
```python
# services/service_factory.py
import os
import asyncpg
from services.zhipu_service import ZhipuAIService
from services.vector_service import VectorService
from services.redis_cache import RedisCache
from services.rag_service import RAGService

class ServiceFactory:
    """æœåŠ¡å·¥å‚ç±»"""

    def __init__(self):
        self._zhipu_service = None
        self._vector_service = None
        self._cache_service = None
        self._rag_service = None
        self._db_pool = None

    async def get_db_pool(self) -> asyncpg.Pool:
        """è·å–æ•°æ®åº“è¿æ¥æ± """
        if not self._db_pool:
            self._db_pool = await asyncpg.create_pool(
                os.getenv('DATABASE_URL'),
                min_size=5,
                max_size=20,
                command_timeout=60
            )
        return self._db_pool

    async def get_zhipu_service(self) -> ZhipuAIService:
        """è·å–æ™ºæ™®æ¸…è¨€æœåŠ¡"""
        if not self._zhipu_service:
            self._zhipu_service = ZhipuAIService(
                api_key=os.getenv('ZHIPU_API_KEY'),
                base_url=os.getenv('ZHIPU_BASE_URL', 'https://open.bigmodel.cn/api/paas/v4')
            )
        return self._zhipu_service

    async def get_cache_service(self) -> RedisCache:
        """è·å–ç¼“å­˜æœåŠ¡"""
        if not self._cache_service:
            self._cache_service = RedisCache(
                redis_url=os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
                default_ttl=int(os.getenv('REDIS_TTL', 3600))
            )
            await self._cache_service.connect()
        return self._cache_service

    async def get_vector_service(self) -> VectorService:
        """è·å–å‘é‡æœåŠ¡"""
        if not self._vector_service:
            pool = await self.get_db_pool()
            self._vector_service = VectorService(pool)
        return self._vector_service

    async def get_rag_service(self) -> RAGService:
        """è·å–RAGæœåŠ¡"""
        if not self._rag_service:
            zhipu_service = await self.get_zhipu_service()
            vector_service = await self.get_vector_service()
            cache_service = await self.get_cache_service()

            self._rag_service = RAGService(
                zhipu_service=zhipu_service,
                vector_service=vector_service,
                cache_service=cache_service
            )
        return self._rag_service

    async def initialize_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        await self.get_db_pool()
        await self.get_zhipu_service()
        await self.get_cache_service()
        await self.get_vector_service()
        await self.get_rag_service()

    async def cleanup_services(self):
        """æ¸…ç†æ‰€æœ‰æœåŠ¡"""
        if self._db_pool:
            await self._db_pool.close()

        if self._cache_service:
            await self._cache_service.disconnect()

        if self._zhipu_service:
            await self._zhipu_service.__aexit__(None, None, None)

# å…¨å±€æœåŠ¡å®ä¾‹
service_factory = ServiceFactory()
```

## 5. å¯åŠ¨å’Œéƒ¨ç½²è„šæœ¬

### 5.1 å¼€å‘ç¯å¢ƒå¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# scripts/start-dev.sh

echo "ğŸš€ å¯åŠ¨RAGç³»ç»Ÿå¼€å‘ç¯å¢ƒ..."

# å¯åŠ¨DockeræœåŠ¡
echo "ğŸ“¦ å¯åŠ¨PostgreSQLå’ŒRedis..."
docker-compose -f docker-compose.dev.yml up -d

# ç­‰å¾…æ•°æ®åº“å¯åŠ¨
echo "â³ ç­‰å¾…æ•°æ®åº“å¯åŠ¨..."
sleep 10

# åˆå§‹åŒ–æ•°æ®åº“
echo "ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“..."
python scripts/init_database.py

# å¯åŠ¨FastAPIåº”ç”¨
echo "ğŸŒ å¯åŠ¨FastAPIåº”ç”¨..."
export APP_ENV=development
uvicorn main:app --reload --host 0.0.0.0 --port 8000

echo "âœ… å¼€å‘ç¯å¢ƒå¯åŠ¨å®Œæˆï¼"
echo "ğŸ“– APIæ–‡æ¡£: http://localhost:8000/docs"
echo "ğŸ”§ PgAdmin: http://localhost:5050"
```

### 5.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy-prod.sh

echo "ğŸš€ éƒ¨ç½²RAGç³»ç»Ÿç”Ÿäº§ç¯å¢ƒ..."

# æ„å»ºDockeré•œåƒ
echo "ğŸ—ï¸ æ„å»ºDockeré•œåƒ..."
docker build -t rag-backend:latest .

# åœæ­¢æ—§å®¹å™¨
echo "ğŸ›‘ åœæ­¢æ—§å®¹å™¨..."
docker-compose -f docker-compose.prod.yml down

# å¯åŠ¨æ–°å®¹å™¨
echo "ğŸš€ å¯åŠ¨ç”Ÿäº§å®¹å™¨..."
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
curl -f http://localhost:8000/health || exit 1

echo "âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å®Œæˆï¼"
```

### 5.3 æ•°æ®åº“è¿ç§»è„šæœ¬
```bash
#!/bin/bash
# scripts/migrate-database.sh

echo "ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“è¿ç§»..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ -z "$DATABASE_URL" ]; then
    echo "âŒ é”™è¯¯ï¼šæœªè®¾ç½®DATABASE_URLç¯å¢ƒå˜é‡"
    exit 1
fi

# æ‰§è¡Œè¿ç§»
python scripts/migrate-data.py

echo "âœ… æ•°æ®åº“è¿ç§»å®Œæˆï¼"
```

## 6. æµ‹è¯•å’ŒéªŒè¯

### 6.1 å•å…ƒæµ‹è¯•
```python
# tests/test_rag_service.py
import pytest
import asyncio
from unittest.mock import AsyncMock, patch
from services.rag_service import RAGService
from services.zhipu_service import ZhipuAIService
from services.vector_service import VectorService
from services.redis_cache import RedisCache

@pytest.fixture
async def rag_service():
    """åˆ›å»ºRAGæœåŠ¡æµ‹è¯•å®ä¾‹"""
    zhipu_service = AsyncMock(spec=ZhipuAIService)
    vector_service = AsyncMock(spec=VectorService)
    cache_service = AsyncMock(spec=RedisCache)

    service = RAGService(zhipu_service, vector_service, cache_service)
    return service

@pytest.mark.asyncio
async def test_search_documents(rag_service):
    """æµ‹è¯•æ–‡æ¡£æœç´¢åŠŸèƒ½"""
    # æ¨¡æ‹Ÿè¿”å›æ•°æ®
    rag_service.vector_service.hybrid_search.return_value = [
        {"document_id": "doc1", "content": "æµ‹è¯•å†…å®¹", "similarity": 0.8}
    ]

    results = await rag_service.search_documents("æµ‹è¯•æŸ¥è¯¢")

    assert len(results) == 1
    assert results[0]["document_id"] == "doc1"
    rag_service.vector_service.hybrid_search.assert_called_once()

@pytest.mark.asyncio
async def test_generate_response(rag_service):
    """æµ‹è¯•å“åº”ç”ŸæˆåŠŸèƒ½"""
    # æ¨¡æ‹Ÿæœç´¢ç»“æœ
    rag_service.search_documents = AsyncMock(return_value=[])

    # æ¨¡æ‹Ÿæ™ºæ™®æ¸…è¨€å“åº”
    rag_service.zhipu_service.chat_completion.return_value = {
        "choices": [{"message": {"content": "æµ‹è¯•å›å¤"}}],
        "model": "glm-4-plus",
        "usage": {"total_tokens": 100}
    }

    response = await rag_service.generate_response(
        query="æµ‹è¯•é—®é¢˜",
        use_context=False
    )

    assert response["response"] == "æµ‹è¯•å›å¤"
    assert response["model"] == "glm-4-plus"
```

### 6.2 é›†æˆæµ‹è¯•
```python
# tests/test_integration.py
import pytest
import asyncio
from httpx import AsyncClient
from main import app

@pytest.mark.asyncio
async def test_chat_integration():
    """æµ‹è¯•å®Œæ•´çš„èŠå¤©æµç¨‹"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # å‘é€èŠå¤©è¯·æ±‚
        response = await client.post(
            "/api/chat",
            json={
                "message": "ä½ å¥½",
                "conversation_id": None,
                "knowledge_base_id": None
            }
        )

        assert response.status_code == 200
        data = response.json()
        assert "response" in data
        assert "sources" in data

@pytest.mark.asyncio
async def test_document_indexing():
    """æµ‹è¯•æ–‡æ¡£ç´¢å¼•æµç¨‹"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # ä¸Šä¼ æ–‡æ¡£
        response = await client.post(
            "/api/documents",
            files={"file": ("test.txt", "æµ‹è¯•æ–‡æ¡£å†…å®¹", "text/plain")},
            data={"knowledge_base_id": "test_kb", "title": "æµ‹è¯•æ–‡æ¡£"}
        )

        assert response.status_code == 200
        data = response.json()
        assert "document_id" in data
```

## 7. ç›‘æ§å’Œæ—¥å¿—

### 7.1 åº”ç”¨ç›‘æ§
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time
import logging

logger = logging.getLogger(__name__)

# å®šä¹‰æŒ‡æ ‡
CHAT_REQUESTS = Counter('rag_chat_requests_total', 'Total chat requests')
CHAT_DURATION = Histogram('rag_chat_duration_seconds', 'Chat response duration')
DOCUMENT_INDEXING = Counter('rag_document_indexing_total', 'Documents indexed')
VECTOR_SEARCH = Histogram('rag_vector_search_duration_seconds', 'Vector search duration')
ACTIVE_CONNECTIONS = Gauge('rag_active_connections', 'Active database connections')

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.chat_requests = CHAT_REQUESTS
        self.chat_duration = CHAT_DURATION
        self.document_indexing = DOCUMENT_INDEXING
        self.vector_search = VECTOR_SEARCH
        self.active_connections = ACTIVE_CONNECTIONS

    def record_chat_request(self):
        """è®°å½•èŠå¤©è¯·æ±‚"""
        self.chat_requests.inc()

    def record_chat_duration(self, duration: float):
        """è®°å½•èŠå¤©å“åº”æ—¶é—´"""
        self.chat_duration.observe(duration)

    def record_document_indexing(self):
        """è®°å½•æ–‡æ¡£ç´¢å¼•"""
        self.document_indexing.inc()

    def record_vector_search(self, duration: float):
        """è®°å½•å‘é‡æœç´¢æ—¶é—´"""
        self.vector_search.observe(duration)

    def set_active_connections(self, count: int):
        """è®¾ç½®æ´»è·ƒè¿æ¥æ•°"""
        self.active_connections.set(count)

    def get_metrics(self) -> str:
        """è·å–Prometheusæ ¼å¼çš„æŒ‡æ ‡"""
        return generate_latest()

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨
metrics = MetricsCollector()
```

### 7.2 æ—¥å¿—é…ç½®
```python
# logging_config.py
import logging
import logging.config
from datetime import datetime

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
        'json': {
            'format': '{"timestamp": "%(asctime)s", "level": "%(levelname)s", "logger": "%(name)s", "message": "%(message)s"}'
        }
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'detailed'
        },
        'file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'DEBUG',
            'filename': 'logs/rag_system.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json'
        },
        'error_file': {
            'class': 'logging.handlers.RotatingFileHandler',
            'level': 'ERROR',
            'filename': 'logs/rag_system_error.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json'
        }
    },
    'loggers': {
        '': {  # root logger
            'handlers': ['console', 'file'],
            'level': 'INFO'
        },
        'rag': {
            'handlers': ['console', 'file', 'error_file'],
            'level': 'DEBUG',
            'propagate': False
        }
    }
}

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    logging.config.dictConfig(LOGGING_CONFIG)
    logger = logging.getLogger(__name__)
    logger.info("Logging system initialized")
```

è¿™ä¸ªå®Œæ•´çš„PostgreSQL + æ™ºæ™®æ¸…è¨€é›†æˆæ–¹æ¡ˆæä¾›äº†ï¼š

1. **å®Œæ•´çš„æ•°æ®åº“æ¶æ„** - åŸºäºPostgreSQL + pgvectorçš„å‘é‡å­˜å‚¨
2. **æ™ºæ™®æ¸…è¨€APIå°è£…** - å®Œæ•´çš„åµŒå…¥å’ŒèŠå¤©æœåŠ¡
3. **Redisç¼“å­˜ç³»ç»Ÿ** - æé«˜æŸ¥è¯¢æ€§èƒ½
4. **æ··åˆæœç´¢åŠŸèƒ½** - å‘é‡æœç´¢å’Œå…³é”®è¯æœç´¢ç»“åˆ
5. **å®Œæ•´çš„è¿ç§»å·¥å…·** - ä»MySQL/FAISSè¿ç§»åˆ°PostgreSQL/pgvector
6. **æœåŠ¡å·¥å‚æ¨¡å¼** - ç»Ÿä¸€ç®¡ç†å„ç§æœåŠ¡
7. **ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ** - å®Œæ•´çš„è¿ç»´æ”¯æŒ
8. **æµ‹è¯•æ¡†æ¶** - å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
9. **éƒ¨ç½²è„šæœ¬** - å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

è¿™ä¸ªæ–¹æ¡ˆæ”¯æŒé«˜å¹¶å‘ã€é«˜å¯ç”¨ï¼Œå¹¶ä¸”å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚
    async with DatabaseMigration(db_url) as migration:
        if input("æ˜¯å¦ä»MySQLè¿ç§»æ•°æ®ï¼Ÿ(y/n): ").lower() == 'y':
            await migration.migrate_from_mysql(mysql_config)
        else:
            print("è·³è¿‡MySQLæ•°æ®è¿ç§»")

        # éªŒè¯è¿ç§»ç»“æœ
        await migration.validate_migration()

async def main():
    """ä¸»å…¥å£å‡½æ•°"""
    await main()

if __name__ == "__main__":
    asyncio.run(main())
```
```
TodoWrite
<arg_key>todos</arg_key>
<arg_value>[{"content": "è®¾è®¡å’Œå®ç°PostgreSQLæ•°æ®åº“è¿ç§»å’Œæ™ºæ™®æ¸…è¨€APIé›†æˆ", "status": "in_progress", "activeForm": "è®¾è®¡å’Œå®ç°PostgreSQLæ•°æ®åº“è¿ç§»å’Œæ™ºæ™®æ¸…è¨€APIé›†æˆ"}, {"content": "PostgreSQLç¯å¢ƒæ­å»º", "status": "completed", "activeForm": "å®ŒæˆPostgreSQLç¯å¢ƒæ­å»º"}, {"content": "pgvectoræ‰©å±•å®‰è£…é…ç½®", "status": "in_progress", "activeForm": "å®‰è£…å’Œé…ç½®pgvectoræ‰©å±•"}, {"content": "æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ç¼–å†™", "status": "pending", "activeForm": "ç¼–å†™æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬"}, {"content": "æ•°æ®è¿ç§»æ–¹æ¡ˆè®¾è®¡", "status": "pending", "activeForm": "è®¾è®¡æ•°æ®è¿ç§»æ–¹æ¡ˆ"}, {"content": "æ•°æ®æ¨¡å‹é‡æ„", "status": "pending", "activeForm": "é‡æ„æ•°æ®æ¨¡å‹ä»¥æ”¯æŒé‚®ç®±è®¤è¯"}, {"content": "æ™ºæ™®æ¸…è¨€APIæ¥å…¥é…ç½®", "status": "pending", "activeForm": "é…ç½®æ™ºæ™®æ¸…è¨€APIæ¥å…¥"}, {"content": "Embeddingå’ŒChatæœåŠ¡å°è£…", "status": "pending", "activeForm": "å°è£…Embeddingå’ŒChatæœåŠ¡"}]